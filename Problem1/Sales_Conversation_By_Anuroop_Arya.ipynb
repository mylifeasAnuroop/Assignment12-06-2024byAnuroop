{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Research Context:**\n",
        "\n",
        "In the field of Language Models and generative AI, the ability to generate coherent and contextually appropriate conversations has become a pivotal area of exploration. This problem statement aims to delve into this arena by challenging participants to create an extensive sales conversation dataset, drawing inspiration from the intriguing research paper, \"Let the LLMs Talk\" (2312.02913 on arXiv.org). This paper illuminates the potential for LLMs to engage in meaningful and diverse conversations.\n",
        "Submitted By - Anuroop Arya"
      ],
      "metadata": {
        "id": "GqyTTg1ZOPjJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2IL0j9vZVLl",
        "outputId": "238ce474-c1f1-4c71-825c-a13a591264d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue saved to generated_dialogue.csv\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2-medium\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the padding token to the end of sentence token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "def generate_reply(prompt, max_tokens=50, temperature=0.9, top_p=0.95):\n",
        "    \"\"\"Generate a reply based on the given prompt using the GPT-2 model.\"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        reply_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(reply_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def start_conversation(prompt, num_turns=20):\n",
        "    \"\"\"Generate a sales conversation with a given number of turns.\"\"\"\n",
        "    dialogue = [(\"Salesperson\", prompt.split(\": \")[1], datetime.now().isoformat())]\n",
        "\n",
        "    for turn in range(num_turns):\n",
        "        last_reply = dialogue[-1][1]\n",
        "        if turn % 2 == 0:\n",
        "            speaker, next_speaker = \"Customer\", \"Salesperson\"\n",
        "        else:\n",
        "            speaker, next_speaker = \"Salesperson\", \"Customer\"\n",
        "\n",
        "        next_prompt = f\"{last_reply}\\n{speaker}:\"\n",
        "        reply = generate_reply(next_prompt)\n",
        "        dialogue.append((speaker, reply.split(\": \")[-1].strip(), datetime.now().isoformat()))\n",
        "\n",
        "    return dialogue\n",
        "\n",
        "def save_dialogue_to_csv(dialogue, filename=\"generated_dialogue.csv\"):\n",
        "    \"\"\"Save the generated dialogue to a CSV file.\"\"\"\n",
        "    dialogue_df = pd.DataFrame(dialogue, columns=[\"Speaker\", \"Text\", \"Timestamp\"])\n",
        "    dialogue_df.to_csv(filename, index=False)\n",
        "    print(f\"Dialogue saved to {filename}\")\n",
        "\n",
        "# Initial prompt for starting a sales conversation\n",
        "initial_prompt = \"Salesperson: Good morning! How can I assist you with your tech needs today?\"\n",
        "dialogue = start_conversation(initial_prompt)\n",
        "\n",
        "# Save the dialogue to a CSV file\n",
        "save_dialogue_to_csv(dialogue)\n",
        "\n"
      ]
    }
  ]
}